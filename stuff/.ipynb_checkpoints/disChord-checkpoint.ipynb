{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DisChord: A Supervised Learning Network for Musical Chord Detection\n",
    "### John Chai, Jaime Pang (M20604)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "blah blah refer to report\n",
    "\n",
    "### Required Packages\n",
    "Librosa\n",
    "sounddevice\n",
    "ipywidget\n",
    "keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Transformation\n",
    "Transposes each score into different keys to get more training data for reference (only run this if labels is still empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import csv\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allKeys(soundfile, csvfile):  # Data augmentation by pitch shifting to every available key\n",
    "    tones = [\"A\", \"A#\", \"B\", \"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"NC\"]\n",
    "    chdict = {\"A\": 0, \"A#\": 1, \"Bb\": 1, \"B\": 2, \"C\": 3, \"C#\": 4, \"Db\": 4, \"D\": 5, \"D#\": 6,\n",
    "              \"Eb\": 6, \"E\": 7, \"F\": 8, \"F#\": 9, \"Gb\": 9, \"G\": 10, \"G#\": 11, \"Ab\": 11, \"NC\": 12}\n",
    "\n",
    "    y, sr = librosa.load(soundfile)\n",
    "    for n in range(-6, 6):\n",
    "        yn = librosa.effects.pitch_shift(y, sr, n_steps=float(n))  # Pitch shifted samples, same sample rate\n",
    "        csvdir, csvname = path.split(csvfile)\n",
    "        print(csvdir)\n",
    "        csvname, csvext = path.splitext(csvname)\n",
    "        with open(csvfile, newline='') as csvf:\n",
    "            r = csv.reader(csvf)\n",
    "            n_csv = path.join('data/labels/', (csvname+'_'+str(n)+csvext))  # writes new labels to data/labels\n",
    "            print(n_csv)\n",
    "            with open(n_csv, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                for row in r:\n",
    "                    row[0] = tones[(chdict.get(row[0])+n) % 12]  # Write transposed chord symbol\n",
    "                    writer.writerow(row)\n",
    "\n",
    "        n_sf = path.join('data/soundfiles/', (csvname + '_' + str(n)+'.wav'))  # wave file stored in data/soundfiles\n",
    "        librosa.output.write_wav(n_sf, yn, sr)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Run this to generate all training data\n",
    "allKeys('data/505.wav', 'data/505.csv')\n",
    "allKeys('data/All_Of_Me.wav', 'data/All_Of_Me.csv')\n",
    "allKeys('data/Ave_Verum_Corpus_Mozart.wav', 'data/Ave_Verum_Corpus_Mozart.csv')\n",
    "allKeys('data/chordtest0.wav', 'data/chordtest0.csv')\n",
    "allKeys('data/despacito.wav', 'data/despacito.csv')\n",
    "allKeys('data/greensleeves.wav', 'data/greensleeves.csv')\n",
    "allKeys('data/losing_my_religion.wav', 'data/losing_my_religion.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing/Feature Extraction\n",
    "Here we define the functions required to process the data from the input csv/wav files, and also extract the features we want for analysis by segmenting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chroma_process(y, sr):\n",
    "    HOP_LENGTH = 512  # Number of samples per window for processing\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y, margin=8)  # Separate track into percussive and harmonic elements\n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, hop_length=HOP_LENGTH)  # Track beats and return frames\n",
    "    # Audio is cut into HOP_LENGTH long frames and indexed from 0, beat onset frame indices are an array\n",
    "\n",
    "    chroma = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr, bins_per_octave=12*3, hop_length=HOP_LENGTH)  # Chroma\n",
    "\n",
    "    chroma_filter = np.minimum(chroma,\n",
    "                               librosa.decompose.nn_filter(chroma,\n",
    "                                                           aggregate=np.median,\n",
    "                                                           metric='cosine'))  # Some filtering to remove noise\n",
    "    chroma_smooth = scipy.ndimage.median_filter(chroma_filter, size=(1, 9))  # More filtering\n",
    "    # print(len(beat_frames))\n",
    "    while beat_frames[-1] < (len(y)//HOP_LENGTH):  # Add extra frames to the end to avoid the end parts being missed\n",
    "        beat_frames = np.append(beat_frames, (beat_frames[-1]+int(np.mean([(beat_frames[i] - beat_frames[i - 1])\n",
    "                                                                           for i in range(1, len(beat_frames))]))))\n",
    "\n",
    "    beat_chroma = librosa.util.sync(chroma_smooth, beat_frames, aggregate=np.median)  # Average chroma each beat\n",
    "    chroma_each_beat = np.transpose(beat_chroma)  # flip array dimensions so each beat has shape (12, )\n",
    "\n",
    "    return chroma_each_beat, librosa.frames_to_time(beat_frames, sr)  # return chroma and beat timings in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_process(label_csv, sound_file, split=False):  # split returns split chroma\n",
    "    print(sound_file)\n",
    "    y0, sr0 = librosa.load(sound_file)\n",
    "    if split:\n",
    "        x_train, beat_frames = chroma_process_split(y0, sr0)\n",
    "    else:\n",
    "        x_train, beat_frames = chroma_process(y0, sr0)\n",
    "    labels = [[\"A\", \"A#\", \"B\", \"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"NC\", \"M\", \"m\"]]  # Possible labels\n",
    "\n",
    "    y_train = np.zeros((len(x_train), len(labels[0])))  # Initialize array\n",
    "    mlb = MultiLabelBinarizer()  # Does multi 1 hot encoding\n",
    "    chdict = {\"A\": 0, \"A#\": 1, \"Bb\": 1, \"B\": 2, \"C\": 3, \"C#\": 4, \"Db\": 4, \"D\": 5, \"D#\": 6,\n",
    "              \"Eb\": 6, \"E\": 7, \"F\": 8, \"F#\": 9, \"Gb\": 9, \"G\": 10, \"G#\": 11, \"Ab\": 11, \"NC\": 12}  # Dict for flat labels\n",
    "    mlb.fit(labels)\n",
    "    with open(label_csv, newline='') as csvfile:  # read csv labels\n",
    "        r = csv.reader(csvfile)\n",
    "        ri = 0  # beat index variable\n",
    "        for row in r:\n",
    "            for i in range(int(row[2])):  # for i number of beats that the chord lasts\n",
    "                try:\n",
    "                    row[0] = labels[0][chdict.get(row[0])]\n",
    "                    y_train[ri] = mlb.transform([tuple(row[:2])])  # one hot encoded label array\n",
    "                    ri += 1\n",
    "                except IndexError:  # Sometimes there are more beats than labels, indexError will happen\n",
    "                    print(ri)\n",
    "                    ri += 1\n",
    "                    pass\n",
    "        print(y_train)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chroma_process_split(y, sr):\n",
    "    HOP_LENGTH = 512\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y, margin=8)  # percussive harmonic split\n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, hop_length=HOP_LENGTH)  # beat frames\n",
    "\n",
    "    chroma = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr, bins_per_octave=12*3, hop_length=HOP_LENGTH)  # chroma\n",
    "\n",
    "    chroma_filter = np.minimum(chroma,\n",
    "                               librosa.decompose.nn_filter(chroma,\n",
    "                                                           aggregate=np.median,\n",
    "                                                           metric='cosine'))\n",
    "    chroma_smooth = scipy.ndimage.median_filter(chroma_filter, size=(1, 9))  # Filtering\n",
    "\n",
    "    while beat_frames[-1] < (len(y)//HOP_LENGTH):  # beat frame padding to avoid beats less than labeled beats\n",
    "        beat_frames = np.append(beat_frames, (beat_frames[-1]+int(np.mean([(beat_frames[i] - beat_frames[i - 1])\n",
    "                                                                           for i in range(1, len(beat_frames))]))))\n",
    "\n",
    "    n_seg = 4  # number of subdivisions per beat, 4 = semiquavers\n",
    "    split_beat_frames = split_beats(beat_frames, n_segments=n_seg)  # beat frames of semiquavers\n",
    "    split_chroma = librosa.util.sync(chroma_smooth, split_beat_frames, aggregate=np.median)\n",
    "\n",
    "    if split_chroma.shape[1] % n_seg != 0:  # Cut off excess chroma if last segment is not 4 semiquavers\n",
    "        split_chroma = split_chroma[:, :(-1*(split_chroma.shape[1] % n_seg))]\n",
    "        beat_frames = beat_frames[:-1]\n",
    "\n",
    "    chroma_each_beat = np.split(split_chroma, (split_chroma.shape[1]/4), axis=1)  # Cut into 4 semiquaver sections\n",
    "    for i, chroma in enumerate(chroma_each_beat):\n",
    "        chroma0 = np.transpose(chroma)  # flip dimensions to get (12, 4) array for each semiquaver\n",
    "        chroma_each_beat[i] = chroma0\n",
    "    chroma_each_beat = np.stack(chroma_each_beat)  # from list of array to array\n",
    "    return chroma_each_beat, librosa.frames_to_time(beat_frames, sr)  # return subdivided chroma and beat timings in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_beats(beats, n_segments=4):  # inserts additional beat indices such that each beat is now n_seg sub beats\n",
    "    new_beats = []\n",
    "    sub = 0\n",
    "    for i in range(len(beats)-1):\n",
    "        space = beats[i+1]-beats[i]  # number of frames in beat\n",
    "        sub = space/n_segments  # number of frames per sub beat (semiquaver by default)\n",
    "        for j in range(n_segments):\n",
    "            new_beats.append(beats[i]+int(j*sub))\n",
    "    for j in range(n_segments):  # fix off by one without index error\n",
    "        new_beats.append(beats[len(beats)-1]+int(j*sub))\n",
    "    new_beats = np.array(new_beats)\n",
    "    return new_beats  # returns array of subdivided beat frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating Model\n",
    "The model is instantiated. In experimentation, we had tried 6 different sets of parameters for models in order to gauge which had the best predictions. It was determined that (5) was the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines all the model architectures\n",
    "def model_0():  # first attempt, using chroma aggregated to beats\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=12, activation='sigmoid', input_shape=(12,)))\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(15, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def model_1():  # second attempt, key only\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=12, activation='sigmoid', input_shape=(12,)))\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(12, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def model_2():  # second attempt, minor/major only\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=12, activation='sigmoid', input_shape=(12,)))\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def model_3():  # third attempt, using semiquaver chroma grouped into beats\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(4, 12)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(15, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def model_4():  # fourth attempt, using semiquaver chroma again, using RNN as it is time series data\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(4, 12)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(15, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def model_5():  # fifth attempt, RNN semiquavers key only\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(4, 12)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(12, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def model_6():  # fifth attempt, RNN semiquavers minor/major only\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(4, 12)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "We pass our training data through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different cases for the different model tests\n",
    "def train(model_id, retrain=False, EPOCHS=50):\n",
    "    \n",
    "    if model_id == 0:\n",
    "        model = model_0()\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        if (not os.path.exists('xt.npy')) or retrain:  # pre processed data not present\n",
    "            xt, yt = np.zeros(12), np.zeros(15)  # initialize x train and y train\n",
    "            donefiles = []\n",
    "            print('die')\n",
    "        else:\n",
    "            donefiles = pickle.load(open('donefiles.pkl', 'rb'))  # load previously preprocessed data\n",
    "            xt, yt = np.load('xt.npy'), np.load('yt.npy')\n",
    "            print(xt.shape)\n",
    "            print(yt.shape)\n",
    "\n",
    "        for (root, dir, file) in os.walk('data/labels/'):\n",
    "            for f in file:\n",
    "                if (f not in donefiles) or retrain:  # process unprocessed data or all data if retrain flag set to true\n",
    "                    x0, y0 = label_process((\"data/labels/\" + f), (\"data/soundfiles/\" + f[:-4] + \".wav\"))\n",
    "                    donefiles.append(f)\n",
    "                    xt = np.vstack([x0, xt])\n",
    "                    yt = np.vstack([y0, yt])\n",
    "\n",
    "        pickle.dump(donefiles, open('donefiles.pkl', 'wb'))  # save list of already processed files\n",
    "        np.save('xt.npy', xt)  # save x train values as numpy binary file\n",
    "        np.save('yt.npy', yt)  # same for y train\n",
    "\n",
    "        # train the network\n",
    "        print(\"[INFO] training network...\")\n",
    "        H = model.fit(\n",
    "            xt, yt, validation_split=0.2,\n",
    "            epochs=EPOCHS, verbose=1)\n",
    "\n",
    "        # save the model to disk\n",
    "        print(\"[INFO] serializing network...\")\n",
    "        model.save(\"model_test.h5\")\n",
    "\n",
    "        # plot the training loss and accuracy\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        N = EPOCHS\n",
    "\n",
    "        plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss_original\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss_original\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc_original\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc_original\")\n",
    "\n",
    "        plt.title(\"Training Loss and Accuracy\")\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Loss/Accuracy\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "    elif model_id == 1:\n",
    "        model = model_1()\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        if (not os.path.exists('xt.npy')) or retrain:\n",
    "            xt, yt = np.zeros(12), np.zeros(15)\n",
    "            donefiles = []\n",
    "            print('die')\n",
    "        else:\n",
    "            donefiles = pickle.load(open('donefiles.pkl', 'rb'))\n",
    "            xt, yt = np.load('xt.npy'), np.load('yt.npy')\n",
    "            print(xt.shape)\n",
    "            print(yt.shape)\n",
    "\n",
    "        for (root, dir, file) in os.walk('data/labels/'):\n",
    "            for f in file:\n",
    "                if (f not in donefiles) or retrain:\n",
    "                    x0, y0 = label_process((\"data/labels/\" + f), (\"data/soundfiles/\" + f[:-4] + \".wav\"))\n",
    "                    donefiles.append(f)\n",
    "                    xt = np.vstack([x0, xt])\n",
    "                    yt = np.vstack([y0, yt])\n",
    "\n",
    "        pickle.dump(donefiles, open('donefiles.pkl', 'wb'))\n",
    "        np.save('xt.npy', xt)\n",
    "        np.save('yt.npy', yt)\n",
    "        yt1 = yt[:, :12]\n",
    "        # train the network\n",
    "        print(\"[INFO] training network...\")\n",
    "        H = model.fit(\n",
    "            xt, yt1, validation_split=0.2,\n",
    "            epochs=EPOCHS, verbose=1)\n",
    "\n",
    "        # save the model to disk\n",
    "        print(\"[INFO] serializing network...\")\n",
    "        model.save(\"model_1_test.h5\")\n",
    "\n",
    "        # plot the training loss and accuracy\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        N = EPOCHS\n",
    "\n",
    "        plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss_key\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss_key\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc_key\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc_key\")\n",
    "\n",
    "        plt.title(\"Training Loss and Accuracy\")\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Loss/Accuracy\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "    elif model_id == 2:\n",
    "        model = model_2()\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        if (not os.path.exists('xt.npy')) or retrain:\n",
    "            xt, yt = np.zeros(12), np.zeros(15)\n",
    "            donefiles = []\n",
    "            print('die')\n",
    "        else:\n",
    "            donefiles = pickle.load(open('donefiles.pkl', 'rb'))\n",
    "            xt, yt = np.load('xt.npy'), np.load('yt.npy')\n",
    "            print(xt.shape)\n",
    "            print(yt.shape)\n",
    "\n",
    "        for (root, dir, file) in os.walk('data/labels/'):\n",
    "            for f in file:\n",
    "                if (f not in donefiles) or retrain:\n",
    "                    x0, y0 = label_process((\"data/labels/\" + f), (\"data/soundfiles/\" + f[:-4] + \".wav\"))\n",
    "                    donefiles.append(f)\n",
    "                    xt = np.vstack([x0, xt])\n",
    "                    yt = np.vstack([y0, yt])\n",
    "\n",
    "        pickle.dump(donefiles, open('donefiles.pkl', 'wb'))\n",
    "        np.save('xt.npy', xt)\n",
    "        np.save('yt.npy', yt)\n",
    "        yt2 = yt[:, 12:]\n",
    "        # train the network\n",
    "        print(\"[INFO] training network...\")\n",
    "        H = model.fit(\n",
    "            xt, yt2, validation_split=0.2,\n",
    "            epochs=EPOCHS, verbose=1)\n",
    "\n",
    "        # save the model to disk\n",
    "        print(\"[INFO] serializing network...\")\n",
    "        model.save(\"model_2_test.h5\")\n",
    "\n",
    "        # plot the training loss and accuracy\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        N = EPOCHS\n",
    "\n",
    "        plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss_mm\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss_mm\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc_mm\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc_mm\")\n",
    "\n",
    "        plt.title(\"Training Loss and Accuracy\")\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Loss/Accuracy\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "    elif model_id == 3 or model_id == 4:\n",
    "        if model_id == 4:\n",
    "            model = model_4()\n",
    "        else:\n",
    "            model = model_3()\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        if (not os.path.exists('xt_alt.npy')) or retrain:\n",
    "            xt, yt = np.zeros((1, 4, 12)), np.zeros(15)\n",
    "            donefiles = []\n",
    "            print('die')\n",
    "        else:\n",
    "            donefiles = pickle.load(open('donefiles_alt.pkl', 'rb'))\n",
    "            xt, yt = np.load('xt_alt.npy'), np.load('yt_alt.npy')\n",
    "            print(xt.shape)\n",
    "            print(yt.shape)\n",
    "\n",
    "        for (root, dir, file) in os.walk('data/labels/'):\n",
    "            for f in file:\n",
    "                if (f not in donefiles) or retrain:\n",
    "                    x0, y0 = label_process((\"data/labels/\" + f), (\"data/soundfiles/\" + f[:-4] + \".wav\"), split=True)\n",
    "                    donefiles.append(f)\n",
    "                    print(xt.shape)\n",
    "                    print(x0.shape)\n",
    "                    xt = np.concatenate((x0, xt))\n",
    "                    yt = np.vstack([y0, yt])\n",
    "\n",
    "        np.set_printoptions(threshold=np.inf)\n",
    "        print(xt[1:21])\n",
    "        print(yt[1:21])\n",
    "        pickle.dump(donefiles, open('donefiles_alt.pkl', 'wb'))\n",
    "        np.save('xt_alt.npy', xt)\n",
    "        np.save('yt_alt.npy', yt)\n",
    "\n",
    "        # train the network\n",
    "        print(\"[INFO] training network...\")\n",
    "        H = model.fit(\n",
    "            xt, yt, validation_split=0.2,\n",
    "            epochs=EPOCHS, verbose=1)\n",
    "\n",
    "        # save the model to disk\n",
    "        print(\"[INFO] serializing network...\")\n",
    "\n",
    "        if model_id == 4:\n",
    "            model.save(\"model_4_test.h5\")\n",
    "        else:\n",
    "            model.save(\"model_3_test.h5\")\n",
    "        # plot the training loss and accuracy\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        N = EPOCHS\n",
    "\n",
    "        plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss_alt\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss_alt\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc_alt\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc_alt\")\n",
    "\n",
    "        plt.title(\"Training Loss and Accuracy\")\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Loss/Accuracy\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "    elif model_id == 5:\n",
    "        model = model_5()\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        if (not os.path.exists('xt_alt.npy')) or retrain:\n",
    "            xt, yt = np.zeros((1, 4, 12)), np.zeros(15)\n",
    "            donefiles = []\n",
    "            print('die')\n",
    "        else:\n",
    "            donefiles = pickle.load(open('donefiles_alt.pkl', 'rb'))\n",
    "            xt, yt = np.load('xt_alt.npy'), np.load('yt_alt.npy')\n",
    "            print(xt.shape)\n",
    "            print(yt.shape)\n",
    "\n",
    "        for (root, dir, file) in os.walk('data/labels/'):\n",
    "            for f in file:\n",
    "                if (f not in donefiles) or retrain:\n",
    "                    x0, y0 = label_process((\"data/labels/\" + f), (\"data/soundfiles/\" + f[:-4] + \".wav\"), split=True)\n",
    "                    donefiles.append(f)\n",
    "                    print(xt.shape)\n",
    "                    print(x0.shape)\n",
    "                    xt = np.concatenate((x0, xt))\n",
    "                    yt = np.vstack([y0, yt])\n",
    "\n",
    "        np.set_printoptions(threshold=np.inf)\n",
    "        pickle.dump(donefiles, open('donefiles_alt.pkl', 'wb'))\n",
    "        np.save('xt_alt.npy', xt)\n",
    "        np.save('yt_alt.npy', yt)\n",
    "        yt1 = yt[:, :12]\n",
    "        # train the network\n",
    "        print(\"[INFO] training network...\")\n",
    "        H = model.fit(\n",
    "            xt, yt1, validation_split=0.2,\n",
    "            epochs=EPOCHS, verbose=1)\n",
    "\n",
    "        # save the model to disk\n",
    "        print(\"[INFO] serializing network...\")\n",
    "        model.save(\"model_5_test.h5\")\n",
    "\n",
    "        # plot the training loss and accuracy\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        N = EPOCHS\n",
    "\n",
    "        plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss_key\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss_key\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc_key\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc_key\")\n",
    "\n",
    "        plt.title(\"Training Loss and Accuracy\")\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Loss/Accuracy\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "    elif model_id == 6:\n",
    "        model = model_6()\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        if (not os.path.exists('xt_alt.npy')) or retrain:\n",
    "            xt, yt = np.zeros((1, 4, 12)), np.zeros(15)\n",
    "            donefiles = []\n",
    "            print('die')\n",
    "        else:\n",
    "            donefiles = pickle.load(open('donefiles_alt.pkl', 'rb'))\n",
    "            xt, yt = np.load('xt_alt.npy'), np.load('yt_alt.npy')\n",
    "            print(xt.shape)\n",
    "            print(yt.shape)\n",
    "\n",
    "        for (root, dir, file) in os.walk('data/labels/'):\n",
    "            for f in file:\n",
    "                if (f not in donefiles) or retrain:\n",
    "                    x0, y0 = label_process((\"data/labels/\" + f), (\"data/soundfiles/\" + f[:-4] + \".wav\"), split=True)\n",
    "                    donefiles.append(f)\n",
    "                    print(xt.shape)\n",
    "                    print(x0.shape)\n",
    "                    xt = np.concatenate((x0, xt))\n",
    "                    yt = np.vstack([y0, yt])\n",
    "\n",
    "        np.set_printoptions(threshold=np.inf)\n",
    "        pickle.dump(donefiles, open('donefiles_alt.pkl', 'wb'))\n",
    "        np.save('xt_alt.npy', xt)\n",
    "        np.save('yt_alt.npy', yt)\n",
    "        yt1 = yt[:, 12:]\n",
    "        # train the network\n",
    "        print(\"[INFO] training network...\")\n",
    "        H = model.fit(\n",
    "            xt, yt1, validation_split=0.2,\n",
    "            epochs=EPOCHS, verbose=1)\n",
    "\n",
    "        # save the model to disk\n",
    "        print(\"[INFO] serializing network...\")\n",
    "        model.save(\"model_6_test.h5\")\n",
    "\n",
    "        # plot the training loss and accuracy\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        N = EPOCHS\n",
    "\n",
    "        plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss_mm\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss_mm\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc_mm\")\n",
    "        plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc_mm\")\n",
    "\n",
    "        plt.title(\"Training Loss and Accuracy\")\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Loss/Accuracy\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32084, 4, 12)\n",
      "(32084, 15)\n",
      "[INFO] training network...\n",
      "WARNING:tensorflow:From C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 25667 samples, validate on 6417 samples\n",
      "Epoch 1/100\n",
      "25667/25667 [==============================] - 11s 437us/step - loss: 0.2113 - accuracy: 0.9260 - val_loss: 0.2756 - val_accuracy: 0.9117\n",
      "Epoch 2/100\n",
      "25667/25667 [==============================] - 10s 384us/step - loss: 0.1733 - accuracy: 0.9357 - val_loss: 0.2641 - val_accuracy: 0.9144\n",
      "Epoch 3/100\n",
      "25667/25667 [==============================] - 13s 526us/step - loss: 0.1656 - accuracy: 0.9388 - val_loss: 0.2564 - val_accuracy: 0.9158\n",
      "Epoch 4/100\n",
      "25667/25667 [==============================] - 11s 437us/step - loss: 0.1599 - accuracy: 0.9410 - val_loss: 0.2561 - val_accuracy: 0.9142\n",
      "Epoch 5/100\n",
      "25667/25667 [==============================] - 11s 420us/step - loss: 0.1557 - accuracy: 0.9428 - val_loss: 0.2563 - val_accuracy: 0.9106\n",
      "Epoch 6/100\n",
      "25667/25667 [==============================] - 12s 449us/step - loss: 0.1519 - accuracy: 0.9443 - val_loss: 0.2528 - val_accuracy: 0.9114\n",
      "Epoch 7/100\n",
      "25667/25667 [==============================] - 11s 424us/step - loss: 0.1489 - accuracy: 0.9458 - val_loss: 0.2607 - val_accuracy: 0.9116\n",
      "Epoch 8/100\n",
      "25667/25667 [==============================] - 11s 432us/step - loss: 0.1462 - accuracy: 0.9468 - val_loss: 0.2615 - val_accuracy: 0.9125\n",
      "Epoch 9/100\n",
      "25667/25667 [==============================] - 12s 484us/step - loss: 0.1438 - accuracy: 0.9477 - val_loss: 0.2684 - val_accuracy: 0.9108\n",
      "Epoch 10/100\n",
      "25667/25667 [==============================] - 12s 457us/step - loss: 0.1415 - accuracy: 0.9483 - val_loss: 0.2730 - val_accuracy: 0.9123\n",
      "Epoch 11/100\n",
      "25667/25667 [==============================] - 11s 421us/step - loss: 0.1397 - accuracy: 0.9491 - val_loss: 0.2740 - val_accuracy: 0.9119\n",
      "Epoch 12/100\n",
      "25667/25667 [==============================] - 11s 431us/step - loss: 0.1378 - accuracy: 0.9498 - val_loss: 0.2736 - val_accuracy: 0.9113\n",
      "Epoch 13/100\n",
      "25667/25667 [==============================] - 11s 426us/step - loss: 0.1362 - accuracy: 0.9503 - val_loss: 0.2804 - val_accuracy: 0.9104\n",
      "Epoch 14/100\n",
      "25667/25667 [==============================] - 11s 421us/step - loss: 0.1344 - accuracy: 0.9512 - val_loss: 0.2749 - val_accuracy: 0.9110\n",
      "Epoch 15/100\n",
      "25667/25667 [==============================] - 11s 425us/step - loss: 0.1334 - accuracy: 0.9516 - val_loss: 0.2845 - val_accuracy: 0.9101\n",
      "Epoch 16/100\n",
      "25667/25667 [==============================] - 12s 471us/step - loss: 0.1314 - accuracy: 0.9520 - val_loss: 0.2878 - val_accuracy: 0.9089\n",
      "Epoch 17/100\n",
      "25667/25667 [==============================] - 11s 413us/step - loss: 0.1301 - accuracy: 0.9525 - val_loss: 0.2875 - val_accuracy: 0.9091\n",
      "Epoch 18/100\n",
      "25667/25667 [==============================] - 10s 408us/step - loss: 0.1290 - accuracy: 0.9531 - val_loss: 0.2849 - val_accuracy: 0.9093\n",
      "Epoch 19/100\n",
      "25667/25667 [==============================] - 10s 407us/step - loss: 0.1279 - accuracy: 0.9531 - val_loss: 0.2975 - val_accuracy: 0.9086\n",
      "Epoch 20/100\n",
      "25667/25667 [==============================] - 11s 410us/step - loss: 0.1265 - accuracy: 0.9536 - val_loss: 0.2869 - val_accuracy: 0.9110\n",
      "Epoch 21/100\n",
      "25667/25667 [==============================] - 12s 473us/step - loss: 0.1254 - accuracy: 0.9543 - val_loss: 0.2976 - val_accuracy: 0.9087\n",
      "Epoch 22/100\n",
      "25667/25667 [==============================] - 11s 438us/step - loss: 0.1236 - accuracy: 0.9547 - val_loss: 0.2967 - val_accuracy: 0.9093\n",
      "Epoch 23/100\n",
      "25667/25667 [==============================] - 11s 432us/step - loss: 0.1226 - accuracy: 0.9551 - val_loss: 0.3025 - val_accuracy: 0.9085\n",
      "Epoch 24/100\n",
      "25667/25667 [==============================] - 11s 427us/step - loss: 0.1221 - accuracy: 0.9551 - val_loss: 0.3007 - val_accuracy: 0.9096\n",
      "Epoch 25/100\n",
      "25667/25667 [==============================] - 11s 428us/step - loss: 0.1205 - accuracy: 0.9557 - val_loss: 0.3059 - val_accuracy: 0.9081\n",
      "Epoch 26/100\n",
      "25667/25667 [==============================] - 11s 425us/step - loss: 0.1195 - accuracy: 0.9561 - val_loss: 0.3059 - val_accuracy: 0.9098\n",
      "Epoch 27/100\n",
      "25667/25667 [==============================] - 11s 421us/step - loss: 0.1188 - accuracy: 0.9565 - val_loss: 0.3101 - val_accuracy: 0.9078\n",
      "Epoch 28/100\n",
      "25667/25667 [==============================] - 11s 427us/step - loss: 0.1170 - accuracy: 0.9570 - val_loss: 0.3160 - val_accuracy: 0.9070\n",
      "Epoch 29/100\n",
      "25667/25667 [==============================] - 11s 422us/step - loss: 0.1163 - accuracy: 0.9571 - val_loss: 0.3130 - val_accuracy: 0.9104\n",
      "Epoch 30/100\n",
      "25667/25667 [==============================] - 11s 421us/step - loss: 0.1156 - accuracy: 0.9573 - val_loss: 0.3085 - val_accuracy: 0.9084\n",
      "Epoch 31/100\n",
      "25667/25667 [==============================] - 11s 425us/step - loss: 0.1145 - accuracy: 0.9575 - val_loss: 0.3209 - val_accuracy: 0.9071\n",
      "Epoch 32/100\n",
      "25667/25667 [==============================] - 11s 419us/step - loss: 0.1134 - accuracy: 0.9580 - val_loss: 0.3213 - val_accuracy: 0.9079\n",
      "Epoch 33/100\n",
      "25667/25667 [==============================] - 11s 426us/step - loss: 0.1126 - accuracy: 0.9582 - val_loss: 0.3115 - val_accuracy: 0.9076\n",
      "Epoch 34/100\n",
      "25667/25667 [==============================] - 11s 423us/step - loss: 0.1116 - accuracy: 0.9584 - val_loss: 0.3238 - val_accuracy: 0.9082\n",
      "Epoch 35/100\n",
      "25667/25667 [==============================] - 11s 423us/step - loss: 0.1108 - accuracy: 0.9590 - val_loss: 0.3243 - val_accuracy: 0.9077\n",
      "Epoch 36/100\n",
      "25667/25667 [==============================] - 11s 426us/step - loss: 0.1097 - accuracy: 0.9591 - val_loss: 0.3237 - val_accuracy: 0.9075\n",
      "Epoch 37/100\n",
      "25667/25667 [==============================] - 11s 420us/step - loss: 0.1090 - accuracy: 0.9594 - val_loss: 0.3366 - val_accuracy: 0.9057\n",
      "Epoch 38/100\n",
      "25667/25667 [==============================] - 11s 420us/step - loss: 0.1077 - accuracy: 0.9596 - val_loss: 0.3260 - val_accuracy: 0.9067\n",
      "Epoch 39/100\n",
      "25667/25667 [==============================] - 11s 423us/step - loss: 0.1070 - accuracy: 0.9601 - val_loss: 0.3350 - val_accuracy: 0.9064\n",
      "Epoch 40/100\n",
      "25667/25667 [==============================] - 11s 420us/step - loss: 0.1063 - accuracy: 0.9603 - val_loss: 0.3450 - val_accuracy: 0.9043\n",
      "Epoch 41/100\n",
      "25667/25667 [==============================] - 11s 422us/step - loss: 0.1047 - accuracy: 0.9607 - val_loss: 0.3422 - val_accuracy: 0.9047\n",
      "Epoch 42/100\n",
      "25667/25667 [==============================] - 11s 426us/step - loss: 0.1044 - accuracy: 0.9609 - val_loss: 0.3446 - val_accuracy: 0.9056\n",
      "Epoch 43/100\n",
      "25667/25667 [==============================] - 11s 426us/step - loss: 0.1034 - accuracy: 0.9612 - val_loss: 0.3469 - val_accuracy: 0.9047\n",
      "Epoch 44/100\n",
      "25667/25667 [==============================] - 11s 439us/step - loss: 0.1026 - accuracy: 0.9617 - val_loss: 0.3412 - val_accuracy: 0.9076\n",
      "Epoch 45/100\n",
      "25667/25667 [==============================] - 12s 471us/step - loss: 0.1016 - accuracy: 0.9618 - val_loss: 0.3523 - val_accuracy: 0.9031\n",
      "Epoch 46/100\n",
      "25667/25667 [==============================] - 12s 466us/step - loss: 0.1009 - accuracy: 0.9622 - val_loss: 0.3565 - val_accuracy: 0.9040\n",
      "Epoch 47/100\n",
      "25667/25667 [==============================] - 12s 462us/step - loss: 0.1003 - accuracy: 0.9620 - val_loss: 0.3563 - val_accuracy: 0.9052\n",
      "Epoch 48/100\n",
      "25667/25667 [==============================] - 13s 498us/step - loss: 0.0994 - accuracy: 0.9621 - val_loss: 0.3591 - val_accuracy: 0.9048\n",
      "Epoch 49/100\n",
      "25667/25667 [==============================] - 12s 484us/step - loss: 0.0983 - accuracy: 0.9632 - val_loss: 0.3638 - val_accuracy: 0.9045\n",
      "Epoch 50/100\n",
      "25667/25667 [==============================] - 14s 543us/step - loss: 0.0974 - accuracy: 0.9631 - val_loss: 0.3704 - val_accuracy: 0.9032\n",
      "Epoch 51/100\n",
      "25667/25667 [==============================] - 12s 483us/step - loss: 0.0967 - accuracy: 0.9633 - val_loss: 0.3699 - val_accuracy: 0.9039: 0.096\n",
      "Epoch 52/100\n",
      "25667/25667 [==============================] - 11s 447us/step - loss: 0.0958 - accuracy: 0.9638 - val_loss: 0.3717 - val_accuracy: 0.9032\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25667/25667 [==============================] - 15s 580us/step - loss: 0.0953 - accuracy: 0.9641 - val_loss: 0.3835 - val_accuracy: 0.9030\n",
      "Epoch 54/100\n",
      "25667/25667 [==============================] - 16s 627us/step - loss: 0.0942 - accuracy: 0.9642 - val_loss: 0.3849 - val_accuracy: 0.9029\n",
      "Epoch 55/100\n",
      "25667/25667 [==============================] - 15s 585us/step - loss: 0.0942 - accuracy: 0.9644 - val_loss: 0.3758 - val_accuracy: 0.9022\n",
      "Epoch 56/100\n",
      "25667/25667 [==============================] - 15s 601us/step - loss: 0.0934 - accuracy: 0.9644 - val_loss: 0.3910 - val_accuracy: 0.9023\n",
      "Epoch 57/100\n",
      "25667/25667 [==============================] - 16s 625us/step - loss: 0.0917 - accuracy: 0.9652 - val_loss: 0.4003 - val_accuracy: 0.9020\n",
      "Epoch 58/100\n",
      "25667/25667 [==============================] - 16s 608us/step - loss: 0.0911 - accuracy: 0.9653 - val_loss: 0.3928 - val_accuracy: 0.9024\n",
      "Epoch 59/100\n",
      "25667/25667 [==============================] - 14s 546us/step - loss: 0.0908 - accuracy: 0.9655 - val_loss: 0.3884 - val_accuracy: 0.9023\n",
      "Epoch 60/100\n",
      "25667/25667 [==============================] - 15s 589us/step - loss: 0.0898 - accuracy: 0.9659 - val_loss: 0.4034 - val_accuracy: 0.9012\n",
      "Epoch 61/100\n",
      "25667/25667 [==============================] - 14s 536us/step - loss: 0.0891 - accuracy: 0.9660 - val_loss: 0.3995 - val_accuracy: 0.9021\n",
      "Epoch 62/100\n",
      "25667/25667 [==============================] - 14s 534us/step - loss: 0.0885 - accuracy: 0.9658 - val_loss: 0.4118 - val_accuracy: 0.9011\n",
      "Epoch 63/100\n",
      "25667/25667 [==============================] - 15s 569us/step - loss: 0.0873 - accuracy: 0.9664 - val_loss: 0.4162 - val_accuracy: 0.9005\n",
      "Epoch 64/100\n",
      "25667/25667 [==============================] - 15s 595us/step - loss: 0.0872 - accuracy: 0.9668 - val_loss: 0.4170 - val_accuracy: 0.9005\n",
      "Epoch 65/100\n",
      "25667/25667 [==============================] - 16s 620us/step - loss: 0.0864 - accuracy: 0.9667 - val_loss: 0.4202 - val_accuracy: 0.9014\n",
      "Epoch 66/100\n",
      "25667/25667 [==============================] - 16s 634us/step - loss: 0.0856 - accuracy: 0.9672 - val_loss: 0.4196 - val_accuracy: 0.9010\n",
      "Epoch 67/100\n",
      "25667/25667 [==============================] - 16s 629us/step - loss: 0.0850 - accuracy: 0.9674 - val_loss: 0.4333 - val_accuracy: 0.9011\n",
      "Epoch 68/100\n",
      "25667/25667 [==============================] - 16s 611us/step - loss: 0.0842 - accuracy: 0.9674 - val_loss: 0.4334 - val_accuracy: 0.8997\n",
      "Epoch 69/100\n",
      "25667/25667 [==============================] - 16s 615us/step - loss: 0.0840 - accuracy: 0.9676 - val_loss: 0.4338 - val_accuracy: 0.8998\n",
      "Epoch 70/100\n",
      "25667/25667 [==============================] - 14s 563us/step - loss: 0.0828 - accuracy: 0.9680 - val_loss: 0.4447 - val_accuracy: 0.8990\n",
      "Epoch 71/100\n",
      "25667/25667 [==============================] - 16s 635us/step - loss: 0.0825 - accuracy: 0.9682 - val_loss: 0.4399 - val_accuracy: 0.9001\n",
      "Epoch 72/100\n",
      "25667/25667 [==============================] - 15s 584us/step - loss: 0.0817 - accuracy: 0.9685 - val_loss: 0.4571 - val_accuracy: 0.9002\n",
      "Epoch 73/100\n",
      "25667/25667 [==============================] - 17s 650us/step - loss: 0.0812 - accuracy: 0.9686 - val_loss: 0.4481 - val_accuracy: 0.8997\n",
      "Epoch 74/100\n",
      "25667/25667 [==============================] - 16s 608us/step - loss: 0.0812 - accuracy: 0.9686 - val_loss: 0.4582 - val_accuracy: 0.9004\n",
      "Epoch 75/100\n",
      "25667/25667 [==============================] - 15s 567us/step - loss: 0.0799 - accuracy: 0.9690 - val_loss: 0.4616 - val_accuracy: 0.8981\n",
      "Epoch 76/100\n",
      "25667/25667 [==============================] - 16s 641us/step - loss: 0.0795 - accuracy: 0.9691 - val_loss: 0.4594 - val_accuracy: 0.8987\n",
      "Epoch 77/100\n",
      "25667/25667 [==============================] - 18s 683us/step - loss: 0.0786 - accuracy: 0.9698 - val_loss: 0.4688 - val_accuracy: 0.8997\n",
      "Epoch 78/100\n",
      "25667/25667 [==============================] - 16s 641us/step - loss: 0.0775 - accuracy: 0.9698 - val_loss: 0.4735 - val_accuracy: 0.8998\n",
      "Epoch 79/100\n",
      "25667/25667 [==============================] - 15s 578us/step - loss: 0.0772 - accuracy: 0.9701 - val_loss: 0.4771 - val_accuracy: 0.8989\n",
      "Epoch 80/100\n",
      "25667/25667 [==============================] - 14s 539us/step - loss: 0.0765 - accuracy: 0.9705 - val_loss: 0.4832 - val_accuracy: 0.8992\n",
      "Epoch 81/100\n",
      "25667/25667 [==============================] - 14s 533us/step - loss: 0.0765 - accuracy: 0.9703 - val_loss: 0.4820 - val_accuracy: 0.8981\n",
      "Epoch 82/100\n",
      "25667/25667 [==============================] - 14s 534us/step - loss: 0.0756 - accuracy: 0.9707 - val_loss: 0.4914 - val_accuracy: 0.8982\n",
      "Epoch 83/100\n",
      "25667/25667 [==============================] - 16s 642us/step - loss: 0.0749 - accuracy: 0.9711 - val_loss: 0.4807 - val_accuracy: 0.8980\n",
      "Epoch 84/100\n",
      "25667/25667 [==============================] - 15s 575us/step - loss: 0.0748 - accuracy: 0.9708 - val_loss: 0.4992 - val_accuracy: 0.8976\n",
      "Epoch 85/100\n",
      "25667/25667 [==============================] - 17s 650us/step - loss: 0.0734 - accuracy: 0.9711 - val_loss: 0.5070 - val_accuracy: 0.8976\n",
      "Epoch 86/100\n",
      "25667/25667 [==============================] - 18s 701us/step - loss: 0.0738 - accuracy: 0.9711 - val_loss: 0.5005 - val_accuracy: 0.8990\n",
      "Epoch 87/100\n",
      "25667/25667 [==============================] - 17s 661us/step - loss: 0.0727 - accuracy: 0.9717 - val_loss: 0.5088 - val_accuracy: 0.8984\n",
      "Epoch 88/100\n",
      "25667/25667 [==============================] - 14s 564us/step - loss: 0.0726 - accuracy: 0.9716 - val_loss: 0.5137 - val_accuracy: 0.8981\n",
      "Epoch 89/100\n",
      "25667/25667 [==============================] - 15s 581us/step - loss: 0.0719 - accuracy: 0.9716 - val_loss: 0.5153 - val_accuracy: 0.8983\n",
      "Epoch 90/100\n",
      "25667/25667 [==============================] - 15s 569us/step - loss: 0.0710 - accuracy: 0.9721 - val_loss: 0.5162 - val_accuracy: 0.8986\n",
      "Epoch 91/100\n",
      "25667/25667 [==============================] - 15s 570us/step - loss: 0.0708 - accuracy: 0.9724 - val_loss: 0.5216 - val_accuracy: 0.8977\n",
      "Epoch 92/100\n",
      "25667/25667 [==============================] - 14s 556us/step - loss: 0.0698 - accuracy: 0.9727 - val_loss: 0.5457 - val_accuracy: 0.8976\n",
      "Epoch 93/100\n",
      "25667/25667 [==============================] - 14s 553us/step - loss: 0.0694 - accuracy: 0.9726 - val_loss: 0.5329 - val_accuracy: 0.8977\n",
      "Epoch 94/100\n",
      "25667/25667 [==============================] - 15s 567us/step - loss: 0.0693 - accuracy: 0.9727 - val_loss: 0.5373 - val_accuracy: 0.8980\n",
      "Epoch 95/100\n",
      "25667/25667 [==============================] - 14s 552us/step - loss: 0.0677 - accuracy: 0.9734 - val_loss: 0.5307 - val_accuracy: 0.8969\n",
      "Epoch 96/100\n",
      "25667/25667 [==============================] - 15s 572us/step - loss: 0.0679 - accuracy: 0.9732 - val_loss: 0.5406 - val_accuracy: 0.8981\n",
      "Epoch 97/100\n",
      "25667/25667 [==============================] - 15s 567us/step - loss: 0.0675 - accuracy: 0.9733 - val_loss: 0.5508 - val_accuracy: 0.8972\n",
      "Epoch 98/100\n",
      "25667/25667 [==============================] - 14s 550us/step - loss: 0.0675 - accuracy: 0.9732 - val_loss: 0.5540 - val_accuracy: 0.8970\n",
      "Epoch 99/100\n",
      "25667/25667 [==============================] - 14s 551us/step - loss: 0.0662 - accuracy: 0.9735 - val_loss: 0.5599 - val_accuracy: 0.8972ss: 0.0\n",
      "Epoch 100/100\n",
      "25667/25667 [==============================] - 14s 556us/step - loss: 0.0659 - accuracy: 0.9739 - val_loss: 0.5605 - val_accuracy: 0.8961\n",
      "[INFO] serializing network...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f07b003535ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Train model 5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-c1ffc52afd7f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model_id, retrain, EPOCHS)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_loss_key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_loss_key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"acc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_acc_key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_acc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_acc_key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Train model 5\n",
    "train(5, EPOCHS=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "We define the classification functions and output that our model will give us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call classify on a loaded sound file to get an array of chord predictions and corresponding timestamps in song\n",
    "#model type is set to 5 by default, can be changed here\n",
    "def classify(y0, sr0, mode=5):\n",
    "\n",
    "    #  Select model based on mode flag, make predictions for sound file\n",
    "    if mode == 0 or mode == 1:\n",
    "\n",
    "        x_data, frame_times = chroma_process(y0, sr0)\n",
    "        if(mode == 0):\n",
    "            model = model_0()\n",
    "            model.load_weights('model_test.h5')\n",
    "            preds = model.predict(x_data)\n",
    "        else:\n",
    "            model0 = model_1()\n",
    "            model1 = model_2()\n",
    "            model0.load_weights('model_1_test.h5')\n",
    "            model1.load_weights('model_2_test.h5')\n",
    "            preds_0 = model0.predict(x_data)\n",
    "            preds_1 = model1.predict(x_data)\n",
    "            preds = np.concatenate([preds_0, preds_1], axis=1)\n",
    "    elif(mode == 3):\n",
    "        x_data, frame_times = chroma_process_split(y0, sr0)\n",
    "        model3 = model_3()\n",
    "        model3.load_weights('model_3_test.h5')\n",
    "        preds = model3.predict(x_data)\n",
    "    elif (mode == 4):\n",
    "        x_data, frame_times = chroma_process_split(y0, sr0)\n",
    "        model4 = model_4()\n",
    "        model4.load_weights('model_4_test.h5')\n",
    "        preds = model4.predict(x_data)\n",
    "    elif (mode==5):\n",
    "        x_data, frame_times = chroma_process_split(y0, sr0)\n",
    "        model0 = model_5()\n",
    "        model1 = model_6()\n",
    "        model0.load_weights('model_5_test.h5')\n",
    "        model1.load_weights('model_6_test.h5')\n",
    "        preds_0 = model0.predict(x_data)\n",
    "        preds_1 = model1.predict(x_data)\n",
    "        preds = np.concatenate([preds_0, preds_1], axis=1)\n",
    "\n",
    "    labels = [[\"A\", \"A#\", \"B\", \"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"NC\", \"M\", \"m\"]]\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit(labels)\n",
    "    pr = np.zeros(preds.shape)\n",
    "    pi = 0\n",
    "    for p in preds:\n",
    "        root = np.where(p == max(p[:12]))  # find index of key with highest confidence\n",
    "        mmn = np.where(p == max(p[12:]))  # same for minor/major\n",
    "        p = np.zeros(p.shape)\n",
    "        p[root] = 1\n",
    "        p[mmn] = 1\n",
    "        pr[pi] = p\n",
    "        pi += 1\n",
    "    label_arr = mlb.inverse_transform(pr)  # returns array of text labels corresponding to prediction\n",
    "    return label_arr, frame_times  # returns labels and corresponding beat timings\n",
    "\n",
    "\n",
    "def classify_to_csv(csv_file, y0, sr0, mode=5):  # exactly what it says\n",
    "    label_arr, frames = classify(y0, sr0, mode=mode)\n",
    "    with open(csv_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for i in range(len(label_arr)):\n",
    "            row = [label_arr[i][0], label_arr[i][1], frames[i]]\n",
    "            print(row)\n",
    "            writer.writerow(row)\n",
    "    print('done')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\librosa\\beat.py:306: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  hop_length=hop_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G', 'M', 0.6965986394557823]\n",
      "['G', 'M', 1.3699773242630386]\n",
      "['A', 'M', 2.020136054421769]\n",
      "['A', 'M', 2.670294784580499]\n",
      "['F', 'm', 3.3436734693877552]\n",
      "['E', 'm', 3.9938321995464854]\n",
      "['E', 'M', 4.6439909297052155]\n",
      "['F', 'M', 5.317369614512471]\n",
      "['C', 'M', 5.990748299319728]\n",
      "['G', 'M', 6.640907029478458]\n",
      "['C', 'M', 7.291065759637188]\n",
      "['C', 'M', 7.9412244897959186]\n",
      "['C', 'm', 8.614603174603175]\n",
      "['G', 'M', 9.264761904761905]\n",
      "['G', 'M', 9.93814058956916]\n",
      "['G', 'M', 10.588299319727891]\n",
      "['F', 'm', 11.261678004535147]\n",
      "['G', 'm', 11.911836734693878]\n",
      "['A', 'M', 12.561995464852608]\n",
      "['A', 'M', 13.235374149659863]\n",
      "['D', 'm', 13.885532879818594]\n",
      "['E', 'M', 14.55891156462585]\n",
      "['G', 'M', 15.209070294784581]\n",
      "['E', 'm', 15.882448979591837]\n",
      "['F', 'M', 16.532607709750568]\n",
      "['C', 'M', 17.182766439909297]\n",
      "['C', 'M', 17.832925170068027]\n",
      "['C', 'M', 18.506303854875284]\n",
      "['F', 'M', 19.156462585034014]\n",
      "['G', 'M', 19.82984126984127]\n",
      "['G', 'M', 20.48]\n",
      "['G', 'M', 21.13015873015873]\n",
      "['G', 'M', 21.803537414965987]\n",
      "['G', 'M', 22.453696145124717]\n",
      "['A', 'M', 23.127074829931974]\n",
      "['A', 'NC', 23.777233560090703]\n",
      "['D', 'm', 24.427392290249433]\n",
      "['E', 'M', 25.10077097505669]\n",
      "['G', 'M', 25.75092970521542]\n",
      "['F', 'M', 26.424308390022677]\n",
      "['C', 'M', 27.074467120181406]\n",
      "['C', 'M', 27.74784580498866]\n",
      "['C', 'M', 28.398004535147393]\n",
      "['C', 'M', 29.048163265306123]\n",
      "['C', 'm', 29.72154195011338]\n",
      "['G', 'M', 30.37170068027211]\n",
      "['G', 'M', 31.02185941043084]\n",
      "['C', 'M', 31.695238095238096]\n",
      "['C', 'M', 32.345396825396826]\n",
      "['C', 'M', 33.01877551020408]\n",
      "['E', 'M', 33.66893424036281]\n",
      "['G', 'M', 34.34231292517007]\n",
      "['G', 'M', 34.9924716553288]\n",
      "['E', 'm', 35.64263038548753]\n",
      "['C', 'M', 36.31600907029478]\n",
      "['C', 'M', 36.96616780045351]\n",
      "['G', 'M', 37.61632653061225]\n",
      "['A', 'm', 38.2897052154195]\n",
      "['C', 'M', 38.963083900226756]\n",
      "['D', 'M', 39.613242630385486]\n",
      "['C', 'M', 40.263401360544215]\n",
      "['F', 'M', 40.91356009070295]\n",
      "['C', 'M', 41.586938775510205]\n",
      "['G', 'M', 42.237097505668935]\n",
      "['G', 'M', 42.887256235827664]\n",
      "['G', 'M', 43.56063492063492]\n",
      "['C', 'M', 44.210793650793654]\n",
      "['G', 'M', 44.88417233560091]\n",
      "['G', 'M', 45.53433106575964]\n",
      "['A', 'M', 46.20770975056689]\n",
      "['A', 'm', 46.85786848072562]\n",
      "['C', 'M', 47.50802721088435]\n",
      "['A', 'M', 48.18140589569161]\n",
      "['G', 'm', 48.83156462585034]\n",
      "['F', 'M', 49.504943310657595]\n",
      "['F', 'M', 50.155102040816324]\n",
      "['C', 'M', 50.80526077097505]\n",
      "['C', 'M', 51.478639455782314]\n",
      "['F', 'M', 52.128798185941044]\n",
      "['G', 'm', 52.77895691609977]\n",
      "['D', 'M', 53.45233560090703]\n",
      "['G', 'M', 54.12571428571429]\n",
      "['G', 'M', 54.77587301587302]\n",
      "['C', 'M', 55.42603174603175]\n",
      "['C', 'M', 56.076190476190476]\n",
      "['A', 'm', 56.74956916099773]\n",
      "['A', 'M', 57.39972789115646]\n",
      "['G', 'M', 58.07310657596372]\n",
      "['G', 'M', 58.72326530612245]\n",
      "['F', 'M', 59.396643990929704]\n",
      "['C', 'M', 60.04680272108843]\n",
      "['C', 'M', 60.69696145124716]\n",
      "['C', 'M', 61.3471201814059]\n",
      "['C', 'M', 62.02049886621315]\n",
      "['C', 'm', 62.69387755102041]\n",
      "['G', 'M', 63.344036281179136]\n",
      "['G', 'M', 63.994195011337865]\n",
      "['C', 'M', 64.6443537414966]\n",
      "['C', 'M', 65.31773242630385]\n",
      "['G', 'm', 65.99111111111111]\n",
      "['G', 'M', 66.64126984126985]\n",
      "['A', 'M', 67.29142857142857]\n",
      "['D', 'M', 67.96480725623583]\n",
      "['C', 'M', 68.61496598639455]\n",
      "['G', 'M', 69.26512471655329]\n",
      "['E', 'm', 69.93850340136055]\n",
      "['F', 'M', 70.58866213151927]\n",
      "['C', 'M', 71.26204081632653]\n",
      "['C', 'M', 71.91219954648525]\n",
      "['C', 'M', 72.56235827664399]\n",
      "['F', 'M', 73.23573696145125]\n",
      "['G', 'M', 73.88589569160997]\n",
      "['G', 'M', 74.55927437641724]\n",
      "['G', 'M', 75.20943310657596]\n",
      "['G', 'M', 75.8595918367347]\n",
      "['G', 'M', 76.53297052154196]\n",
      "['G', 'M', 77.18312925170068]\n",
      "['A', 'M', 77.83328798185941]\n",
      "['D', 'm', 78.50666666666666]\n",
      "['E', 'm', 79.18004535147392]\n",
      "['G', 'M', 79.83020408163266]\n",
      "['F', 'M', 80.48036281179138]\n",
      "['C', 'M', 81.15374149659864]\n",
      "['C', 'M', 81.80390022675736]\n",
      "['C', 'M', 82.4540589569161]\n",
      "['C', 'M', 83.10421768707484]\n",
      "['C', 'm', 83.77759637188208]\n",
      "['G', 'M', 84.45097505668934]\n",
      "['G', 'M', 85.10113378684807]\n",
      "['C', 'M', 85.7512925170068]\n",
      "['E', 'm', 86.42467120181406]\n",
      "['G', 'M', 87.07482993197279]\n",
      "['G', 'M', 87.72498866213152]\n",
      "['A', 'm', 88.39836734693877]\n",
      "['F', 'M', 89.07174603174603]\n",
      "['B', 'm', 89.72190476190477]\n",
      "['E', 'M', 90.37206349206349]\n",
      "['C', 'M', 91.04544217687075]\n",
      "['C', 'M', 91.69560090702947]\n",
      "['C', 'M', 92.34575963718821]\n",
      "['C', 'M', 93.01913832199547]\n",
      "['C', 'M', 93.66929705215419]\n",
      "['C', 'm', 94.31945578231293]\n",
      "['G', 'NC', 94.99283446712018]\n",
      "['G', 'M', 95.64299319727891]\n",
      "['C', 'M', 96.29315192743765]\n",
      "['C', 'M', 96.94331065759637]\n",
      "['C', 'M', 97.61668934240363]\n",
      "['E', 'M', 98.29006802721088]\n",
      "['G', 'M', 98.94022675736962]\n",
      "['G', 'M', 99.59038548752835]\n",
      "['E', 'm', 100.2637641723356]\n",
      "['C', 'M', 100.93714285714286]\n",
      "['C', 'M', 101.58730158730158]\n",
      "['G', 'M', 102.23746031746032]\n",
      "['A', 'm', 102.91083900226758]\n",
      "['C', 'M', 103.5609977324263]\n",
      "['D', 'M', 104.21115646258504]\n",
      "['C', 'M', 104.88453514739228]\n",
      "['F', 'M', 105.53469387755102]\n",
      "['C', 'M', 106.20807256235828]\n",
      "['G', 'M', 106.858231292517]\n",
      "['C', 'M', 107.50839002267574]\n",
      "['G', 'M', 108.18176870748299]\n",
      "['G', 'M', 108.83192743764172]\n",
      "['G', 'M', 109.50530612244899]\n",
      "['A', 'M', 110.15546485260771]\n",
      "['A', 'M', 110.80562358276644]\n",
      "['B', 'm', 111.47900226757369]\n",
      "['E', 'M', 112.12916099773243]\n",
      "['A', 'M', 112.77931972789116]\n",
      "['G', 'M', 113.45269841269841]\n",
      "['F', 'M', 114.12607709750567]\n",
      "['G', 'M', 114.7762358276644]\n",
      "['C', 'M', 115.42639455782313]\n",
      "['C', 'M', 116.07655328798187]\n",
      "['C', 'M', 116.74993197278911]\n",
      "['G', 'M', 117.40009070294785]\n",
      "['G', 'M', 118.05024943310657]\n",
      "['G', 'M', 118.72362811791383]\n",
      "['G', 'M', 119.37378684807257]\n",
      "['C', 'M', 120.04716553287982]\n",
      "['C', 'M', 120.69732426303855]\n",
      "['A', 'm', 121.34748299319727]\n",
      "['A', 'm', 122.02086167800454]\n",
      "['A', 'M', 122.6942403628118]\n",
      "['G', 'M', 123.34439909297052]\n",
      "['F', 'M', 123.99455782312926]\n",
      "['C', 'M', 124.6679365079365]\n",
      "['C', 'M', 125.31809523809524]\n",
      "['C', 'M', 125.96825396825396]\n",
      "['C', 'M', 126.6184126984127]\n",
      "['C', 'M', 127.29179138321996]\n",
      "['G', 'M', 127.9651700680272]\n",
      "['G', 'M', 128.61532879818594]\n",
      "['C', 'M', 129.26548752834466]\n",
      "['C', 'M', 129.93886621315193]\n",
      "['G', 'm', 130.58902494331065]\n",
      "['G', 'M', 131.2624036281179]\n",
      "['F', 'M', 131.91256235827663]\n",
      "['C', 'm', 132.56272108843538]\n",
      "['E', 'M', 133.23609977324264]\n",
      "['G', 'M', 133.88625850340136]\n",
      "['E', 'm', 134.55963718820863]\n",
      "['F', 'M', 135.20979591836735]\n",
      "['C', 'M', 135.85995464852607]\n",
      "['C', 'M', 136.53333333333333]\n",
      "['C', 'M', 137.18349206349205]\n",
      "['F', 'M', 137.85687074829931]\n",
      "['G', 'M', 138.50702947845804]\n",
      "['E', 'm', 139.1571882086168]\n",
      "['C', 'M', 139.83056689342405]\n",
      "['G', 'M', 140.48072562358277]\n",
      "['G', 'M', 141.15410430839003]\n",
      "['G', 'M', 141.80426303854875]\n",
      "['A', 'M', 142.45442176870748]\n",
      "['D', 'm', 143.12780045351474]\n",
      "['E', 'm', 143.77795918367346]\n",
      "['A#', 'm', 144.45133786848072]\n",
      "['C', 'M', 145.10149659863944]\n",
      "['C', 'M', 145.7516553287982]\n",
      "['C', 'M', 146.42503401360545]\n",
      "['C', 'M', 147.07519274376418]\n",
      "['F', 'M', 147.7253514739229]\n",
      "['C', 'm', 148.39873015873016]\n",
      "['G', 'M', 149.04888888888888]\n",
      "['G', 'M', 149.72226757369614]\n",
      "['C', 'M', 150.37242630385487]\n",
      "['E', 'm', 151.04580498866213]\n",
      "['G', 'M', 151.69596371882085]\n",
      "['G', 'M', 152.3461224489796]\n",
      "['A', 'm', 153.01950113378686]\n",
      "['F', 'M', 153.66965986394558]\n",
      "['B', 'm', 154.3198185941043]\n",
      "['E', 'M', 154.99319727891157]\n",
      "['C', 'm', 155.66657596371883]\n",
      "['C', 'M', 156.31673469387755]\n",
      "['C', 'M', 156.96689342403627]\n",
      "['C', 'M', 157.61705215419502]\n",
      "['C', 'M', 158.29043083900225]\n",
      "['C', 'm', 158.940589569161]\n",
      "['G', 'NC', 159.61396825396827]\n",
      "['G', 'M', 160.264126984127]\n",
      "['C', 'M', 160.9142857142857]\n",
      "['C', 'M', 161.58766439909297]\n",
      "['A', 'M', 162.2378231292517]\n",
      "['D', 'M', 162.88798185941044]\n",
      "['A', 'M', 163.56136054421768]\n",
      "['A', 'M', 164.21151927437643]\n",
      "['A', 'm', 164.86167800453515]\n",
      "['E', 'm', 165.5350566893424]\n",
      "['G', 'M', 166.18521541950113]\n",
      "['F', 'M', 166.8585941043084]\n",
      "['F', 'M', 167.50875283446712]\n",
      "['F', 'M', 168.15891156462584]\n",
      "['C', 'M', 168.8322902494331]\n",
      "['C', 'M', 169.48244897959182]\n",
      "['A', 'M', 170.13260770975057]\n",
      "['G', 'M', 170.80598639455783]\n",
      "['C', 'M', 171.45614512471656]\n",
      "['C', 'M', 172.10630385487528]\n",
      "['C', 'M', 172.756462585034]\n",
      "['G', 'M', 173.40662131519275]\n",
      "['C', 'M', 174.05678004535147]\n",
      "['C', 'M', 174.7069387755102]\n",
      "['C', 'M', 175.35709750566895]\n",
      "['D', 'NC', 176.00725623582767]\n",
      "['G', 'm', 176.6574149659864]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#predict from the test set\n",
    "y, sr = librosa.load('test/Maroon_5_-_Memories.wav')\n",
    "classify_to_csv('preds.csv', y, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\librosa\\beat.py:306: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  hop_length=hop_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "['G', 'M', 0.6965986394557823]\n",
      "['G', 'M', 1.3699773242630386]\n",
      "['A', 'M', 2.020136054421769]\n",
      "['A', 'M', 2.670294784580499]\n",
      "['F', 'm', 3.3436734693877552]\n",
      "['E', 'm', 3.9938321995464854]\n",
      "['E', 'M', 4.6439909297052155]\n",
      "['F', 'M', 5.317369614512471]\n",
      "['C', 'M', 5.990748299319728]\n",
      "['G', 'M', 6.640907029478458]\n",
      "['C', 'M', 7.291065759637188]\n",
      "['C', 'M', 7.9412244897959186]\n",
      "['C', 'm', 8.614603174603175]\n",
      "['G', 'M', 9.264761904761905]\n",
      "['G', 'M', 9.93814058956916]\n",
      "['G', 'M', 10.588299319727891]\n",
      "['F', 'm', 11.261678004535147]\n",
      "['G', 'm', 11.911836734693878]\n",
      "['A', 'M', 12.561995464852608]\n",
      "['A', 'M', 13.235374149659863]\n",
      "['D', 'm', 13.885532879818594]\n",
      "['E', 'M', 14.55891156462585]\n",
      "['G', 'M', 15.209070294784581]\n",
      "['E', 'm', 15.882448979591837]\n",
      "['F', 'M', 16.532607709750568]\n",
      "['C', 'M', 17.182766439909297]\n",
      "['C', 'M', 17.832925170068027]\n",
      "['C', 'M', 18.506303854875284]\n",
      "['F', 'M', 19.156462585034014]\n",
      "['G', 'M', 19.82984126984127]\n",
      "['G', 'M', 20.48]\n",
      "['G', 'M', 21.13015873015873]\n",
      "['G', 'M', 21.803537414965987]\n",
      "['G', 'M', 22.453696145124717]\n",
      "['A', 'M', 23.127074829931974]\n",
      "['A', 'NC', 23.777233560090703]\n",
      "['D', 'm', 24.427392290249433]\n",
      "['E', 'M', 25.10077097505669]\n",
      "['G', 'M', 25.75092970521542]\n",
      "['F', 'M', 26.424308390022677]\n",
      "['C', 'M', 27.074467120181406]\n",
      "['C', 'M', 27.74784580498866]\n",
      "['C', 'M', 28.398004535147393]\n",
      "['C', 'M', 29.048163265306123]\n",
      "['C', 'm', 29.72154195011338]\n",
      "['G', 'M', 30.37170068027211]\n",
      "['G', 'M', 31.02185941043084]\n",
      "['C', 'M', 31.695238095238096]\n",
      "['C', 'M', 32.345396825396826]\n",
      "['C', 'M', 33.01877551020408]\n",
      "['E', 'M', 33.66893424036281]\n",
      "['G', 'M', 34.34231292517007]\n",
      "['G', 'M', 34.9924716553288]\n",
      "['E', 'm', 35.64263038548753]\n",
      "['C', 'M', 36.31600907029478]\n",
      "['C', 'M', 36.96616780045351]\n",
      "['G', 'M', 37.61632653061225]\n",
      "['A', 'm', 38.2897052154195]\n",
      "['C', 'M', 38.963083900226756]\n",
      "['D', 'M', 39.613242630385486]\n",
      "['C', 'M', 40.263401360544215]\n",
      "['F', 'M', 40.91356009070295]\n",
      "['C', 'M', 41.586938775510205]\n",
      "['G', 'M', 42.237097505668935]\n",
      "['G', 'M', 42.887256235827664]\n",
      "['G', 'M', 43.56063492063492]\n",
      "['C', 'M', 44.210793650793654]\n",
      "['G', 'M', 44.88417233560091]\n",
      "['G', 'M', 45.53433106575964]\n",
      "['A', 'M', 46.20770975056689]\n",
      "['A', 'm', 46.85786848072562]\n",
      "['C', 'M', 47.50802721088435]\n",
      "['A', 'M', 48.18140589569161]\n",
      "['G', 'm', 48.83156462585034]\n",
      "['F', 'M', 49.504943310657595]\n",
      "['F', 'M', 50.155102040816324]\n",
      "['C', 'M', 50.80526077097505]\n",
      "['C', 'M', 51.478639455782314]\n",
      "['F', 'M', 52.128798185941044]\n",
      "['G', 'm', 52.77895691609977]\n",
      "['D', 'M', 53.45233560090703]\n",
      "['G', 'M', 54.12571428571429]\n",
      "['G', 'M', 54.77587301587302]\n",
      "['C', 'M', 55.42603174603175]\n",
      "['C', 'M', 56.076190476190476]\n",
      "['A', 'm', 56.74956916099773]\n",
      "['A', 'M', 57.39972789115646]\n",
      "['G', 'M', 58.07310657596372]\n",
      "['G', 'M', 58.72326530612245]\n",
      "['F', 'M', 59.396643990929704]\n",
      "['C', 'M', 60.04680272108843]\n",
      "['C', 'M', 60.69696145124716]\n",
      "['C', 'M', 61.3471201814059]\n",
      "['C', 'M', 62.02049886621315]\n",
      "['C', 'm', 62.69387755102041]\n",
      "['G', 'M', 63.344036281179136]\n",
      "['G', 'M', 63.994195011337865]\n",
      "['C', 'M', 64.6443537414966]\n",
      "['C', 'M', 65.31773242630385]\n",
      "['G', 'm', 65.99111111111111]\n",
      "['G', 'M', 66.64126984126985]\n",
      "['A', 'M', 67.29142857142857]\n",
      "['D', 'M', 67.96480725623583]\n",
      "['C', 'M', 68.61496598639455]\n",
      "['G', 'M', 69.26512471655329]\n",
      "['E', 'm', 69.93850340136055]\n",
      "['F', 'M', 70.58866213151927]\n",
      "['C', 'M', 71.26204081632653]\n",
      "['C', 'M', 71.91219954648525]\n",
      "['C', 'M', 72.56235827664399]\n",
      "['F', 'M', 73.23573696145125]\n",
      "['G', 'M', 73.88589569160997]\n",
      "['G', 'M', 74.55927437641724]\n",
      "['G', 'M', 75.20943310657596]\n",
      "['G', 'M', 75.8595918367347]\n",
      "['G', 'M', 76.53297052154196]\n",
      "['G', 'M', 77.18312925170068]\n",
      "['A', 'M', 77.83328798185941]\n",
      "['D', 'm', 78.50666666666666]\n",
      "['E', 'm', 79.18004535147392]\n",
      "['G', 'M', 79.83020408163266]\n",
      "['F', 'M', 80.48036281179138]\n",
      "['C', 'M', 81.15374149659864]\n",
      "['C', 'M', 81.80390022675736]\n",
      "['C', 'M', 82.4540589569161]\n",
      "['C', 'M', 83.10421768707484]\n",
      "['C', 'm', 83.77759637188208]\n",
      "['G', 'M', 84.45097505668934]\n",
      "['G', 'M', 85.10113378684807]\n",
      "['C', 'M', 85.7512925170068]\n",
      "['E', 'm', 86.42467120181406]\n",
      "['G', 'M', 87.07482993197279]\n",
      "['G', 'M', 87.72498866213152]\n",
      "['A', 'm', 88.39836734693877]\n",
      "['F', 'M', 89.07174603174603]\n",
      "['B', 'm', 89.72190476190477]\n",
      "['E', 'M', 90.37206349206349]\n",
      "['C', 'M', 91.04544217687075]\n",
      "['C', 'M', 91.69560090702947]\n",
      "['C', 'M', 92.34575963718821]\n",
      "['C', 'M', 93.01913832199547]\n",
      "['C', 'M', 93.66929705215419]\n",
      "['C', 'm', 94.31945578231293]\n",
      "['G', 'NC', 94.99283446712018]\n",
      "['G', 'M', 95.64299319727891]\n",
      "['C', 'M', 96.29315192743765]\n",
      "['C', 'M', 96.94331065759637]\n",
      "['C', 'M', 97.61668934240363]\n",
      "['E', 'M', 98.29006802721088]\n",
      "['G', 'M', 98.94022675736962]\n",
      "['G', 'M', 99.59038548752835]\n",
      "['E', 'm', 100.2637641723356]\n",
      "['C', 'M', 100.93714285714286]\n",
      "['C', 'M', 101.58730158730158]\n",
      "['G', 'M', 102.23746031746032]\n",
      "['A', 'm', 102.91083900226758]\n",
      "['C', 'M', 103.5609977324263]\n",
      "['D', 'M', 104.21115646258504]\n",
      "['C', 'M', 104.88453514739228]\n",
      "['F', 'M', 105.53469387755102]\n",
      "['C', 'M', 106.20807256235828]\n",
      "['G', 'M', 106.858231292517]\n",
      "['C', 'M', 107.50839002267574]\n",
      "['G', 'M', 108.18176870748299]\n",
      "['G', 'M', 108.83192743764172]\n",
      "['G', 'M', 109.50530612244899]\n",
      "['A', 'M', 110.15546485260771]\n",
      "['A', 'M', 110.80562358276644]\n",
      "['B', 'm', 111.47900226757369]\n",
      "['E', 'M', 112.12916099773243]\n",
      "['A', 'M', 112.77931972789116]\n",
      "['G', 'M', 113.45269841269841]\n",
      "['F', 'M', 114.12607709750567]\n",
      "['G', 'M', 114.7762358276644]\n",
      "['C', 'M', 115.42639455782313]\n",
      "['C', 'M', 116.07655328798187]\n",
      "['C', 'M', 116.74993197278911]\n",
      "['G', 'M', 117.40009070294785]\n",
      "['G', 'M', 118.05024943310657]\n",
      "['G', 'M', 118.72362811791383]\n",
      "['G', 'M', 119.37378684807257]\n",
      "['C', 'M', 120.04716553287982]\n",
      "['C', 'M', 120.69732426303855]\n",
      "['A', 'm', 121.34748299319727]\n",
      "['A', 'm', 122.02086167800454]\n",
      "['A', 'M', 122.6942403628118]\n",
      "['G', 'M', 123.34439909297052]\n",
      "['F', 'M', 123.99455782312926]\n",
      "['C', 'M', 124.6679365079365]\n",
      "['C', 'M', 125.31809523809524]\n",
      "['C', 'M', 125.96825396825396]\n",
      "['C', 'M', 126.6184126984127]\n",
      "['C', 'M', 127.29179138321996]\n",
      "['G', 'M', 127.9651700680272]\n",
      "['G', 'M', 128.61532879818594]\n",
      "['C', 'M', 129.26548752834466]\n",
      "['C', 'M', 129.93886621315193]\n",
      "['G', 'm', 130.58902494331065]\n",
      "['G', 'M', 131.2624036281179]\n",
      "['F', 'M', 131.91256235827663]\n",
      "['C', 'm', 132.56272108843538]\n",
      "['E', 'M', 133.23609977324264]\n",
      "['G', 'M', 133.88625850340136]\n",
      "['E', 'm', 134.55963718820863]\n",
      "['F', 'M', 135.20979591836735]\n",
      "['C', 'M', 135.85995464852607]\n",
      "['C', 'M', 136.53333333333333]\n",
      "['C', 'M', 137.18349206349205]\n",
      "['F', 'M', 137.85687074829931]\n",
      "['G', 'M', 138.50702947845804]\n",
      "['E', 'm', 139.1571882086168]\n",
      "['C', 'M', 139.83056689342405]\n",
      "['G', 'M', 140.48072562358277]\n",
      "['G', 'M', 141.15410430839003]\n",
      "['G', 'M', 141.80426303854875]\n",
      "['A', 'M', 142.45442176870748]\n",
      "['D', 'm', 143.12780045351474]\n",
      "['E', 'm', 143.77795918367346]\n",
      "['A#', 'm', 144.45133786848072]\n",
      "['C', 'M', 145.10149659863944]\n",
      "['C', 'M', 145.7516553287982]\n",
      "['C', 'M', 146.42503401360545]\n",
      "['C', 'M', 147.07519274376418]\n",
      "['F', 'M', 147.7253514739229]\n",
      "['C', 'm', 148.39873015873016]\n",
      "['G', 'M', 149.04888888888888]\n",
      "['G', 'M', 149.72226757369614]\n",
      "['C', 'M', 150.37242630385487]\n",
      "['E', 'm', 151.04580498866213]\n",
      "['G', 'M', 151.69596371882085]\n",
      "['G', 'M', 152.3461224489796]\n",
      "['A', 'm', 153.01950113378686]\n",
      "['F', 'M', 153.66965986394558]\n",
      "['B', 'm', 154.3198185941043]\n",
      "['E', 'M', 154.99319727891157]\n",
      "['C', 'm', 155.66657596371883]\n",
      "['C', 'M', 156.31673469387755]\n",
      "['C', 'M', 156.96689342403627]\n",
      "['C', 'M', 157.61705215419502]\n",
      "['C', 'M', 158.29043083900225]\n",
      "['C', 'm', 158.940589569161]\n",
      "['G', 'NC', 159.61396825396827]\n",
      "['G', 'M', 160.264126984127]\n",
      "['C', 'M', 160.9142857142857]\n",
      "['C', 'M', 161.58766439909297]\n",
      "['A', 'M', 162.2378231292517]\n",
      "['D', 'M', 162.88798185941044]\n",
      "['A', 'M', 163.56136054421768]\n",
      "['A', 'M', 164.21151927437643]\n",
      "['A', 'm', 164.86167800453515]\n",
      "['E', 'm', 165.5350566893424]\n",
      "['G', 'M', 166.18521541950113]\n",
      "['F', 'M', 166.8585941043084]\n",
      "['F', 'M', 167.50875283446712]\n",
      "['F', 'M', 168.15891156462584]\n",
      "['C', 'M', 168.8322902494331]\n",
      "['C', 'M', 169.48244897959182]\n",
      "['A', 'M', 170.13260770975057]\n",
      "['G', 'M', 170.80598639455783]\n",
      "['C', 'M', 171.45614512471656]\n",
      "['C', 'M', 172.10630385487528]\n",
      "['C', 'M', 172.756462585034]\n",
      "['G', 'M', 173.40662131519275]\n",
      "['C', 'M', 174.05678004535147]\n",
      "['C', 'M', 174.7069387755102]\n",
      "['C', 'M', 175.35709750566895]\n",
      "['D', 'NC', 176.00725623582767]\n",
      "['G', 'm', 176.6574149659864]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import time\n",
    "import librosa\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For playing sounds synchronously with chord label printing\n",
    "def play_predicts(file, mode=5, fast=False):  # plays predictions with live chord output\n",
    "    y, sr = librosa.load(file)\n",
    "    labels, beats = classify(y, sr, mode)\n",
    "    cbeat = 0\n",
    "    print(len(beats))\n",
    "    if(fast):\n",
    "        for c in range(len(labels)):\n",
    "            try:\n",
    "                print(\"pred: \" + str(labels[c]))\n",
    "            except IndexError:\n",
    "                pass\n",
    "        return\n",
    "    st = time.time()\n",
    "    sd.play(y, sr)\n",
    "    while(time.time()<st+beats[-1]):\n",
    "        if(time.time()>st+beats[cbeat]):\n",
    "            try:\n",
    "                print(labels[cbeat])\n",
    "            except IndexError:\n",
    "                pass\n",
    "            cbeat += 1\n",
    "    sd.stop()\n",
    "    return\n",
    "\n",
    "\n",
    "def play_check(sf, cf, fast=False, mode=5):  # plays & prints predicts along actual labels for verification.\n",
    "    labels = []\n",
    "    with open(cf, newline='') as csvfile:\n",
    "        r = csv.reader(csvfile)\n",
    "        for row in r:\n",
    "            for i in range(int(row[2])):\n",
    "                # y_train_key[ri][chdict.get(row[0])] = 1\n",
    "                # y_train_min_maj[ri][mmdict.get(row[1])] = 1\n",
    "                # print(row[:2])\n",
    "                labels.append(tuple(row[:2]))\n",
    "    y, sr = librosa.load(sf)\n",
    "    labels0, beats0 = classify(y, sr, mode)\n",
    "\n",
    "    HOP_LENGTH = 512\n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, hop_length=HOP_LENGTH)\n",
    "    while (beat_frames[-1] < (len(y) // HOP_LENGTH)):\n",
    "        beat_frames = np.append(beat_frames, (beat_frames[-1] + int(\n",
    "            np.mean([(beat_frames[i] - beat_frames[i - 1]) for i in range(1, len(beat_frames))]))))\n",
    "    beats = librosa.frames_to_time(beat_frames, sr)\n",
    "    cbeat = 0  # current beat\n",
    "    print(len(beats0))\n",
    "    st = time.time()\n",
    "    if not fast:\n",
    "        sd.play(y, sr)\n",
    "        while (time.time() < st + beats[-1]):\n",
    "            if (time.time() > st + beats[cbeat]):  # if we are in a new beat\n",
    "                try:\n",
    "                    print(\"actual: \" + str(labels[cbeat]) + \" / pred:\" + str(labels0[cbeat]))  # print next label\n",
    "                except IndexError:\n",
    "                    pass\n",
    "                cbeat += 1  # increment current beat\n",
    "        sd.stop()\n",
    "    else:\n",
    "        for c in range(len(labels)):\n",
    "            try:\n",
    "                print(\"actual: \" + str(labels[c]) + \" / pred:\" + str(labels0[c]))\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    print(len(labels))  # number of labels\n",
    "    # print number of fully correct labels\n",
    "    print(sum([labels[i] == labels0[i] for i in range(min([len(labels), len(labels0)]))]))\n",
    "    return\n",
    "\n",
    "\n",
    "def play_csv(sf, cf):  # play sound file with actual csv labels (check hand labeling)\n",
    "    labels = []\n",
    "    sum_beats = 0\n",
    "    with open(cf, newline='') as csvfile:\n",
    "        r = csv.reader(csvfile)\n",
    "        for row in r:\n",
    "            sum_beats += int(row[2])\n",
    "            for i in range(int(row[2])):\n",
    "                # y_train_key[ri][chdict.get(row[0])] = 1\n",
    "                # y_train_min_maj[ri][mmdict.get(row[1])] = 1\n",
    "                # print(row[:2])\n",
    "                labels.append(tuple(row[:2]))\n",
    "    y, sr = librosa.load(sf)\n",
    "    HOP_LENGTH = 512\n",
    "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr, hop_length=HOP_LENGTH)\n",
    "    while (beat_frames[-1] < (len(y) // HOP_LENGTH)):\n",
    "        beat_frames = np.append(beat_frames, (beat_frames[-1] + int(\n",
    "            np.mean([(beat_frames[i] - beat_frames[i - 1]) for i in range(1, len(beat_frames))]))))\n",
    "    beats = librosa.frames_to_time(beat_frames, sr)\n",
    "    cbeat = 0\n",
    "    print(len(beats))\n",
    "    print(sum_beats)\n",
    "    st = time.time()\n",
    "    sd.play(y, sr)\n",
    "    while (time.time() < st + beats[-1]):\n",
    "        if (time.time() > st + beats[cbeat]):\n",
    "            try:\n",
    "                print(\"actual: \" + str(labels[cbeat]))\n",
    "            except IndexError:\n",
    "                pass\n",
    "            cbeat += 1\n",
    "    sd.stop()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaime\\Anaconda3\\envs\\newenv1\\lib\\site-packages\\librosa\\beat.py:306: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  hop_length=hop_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('A', 'M')\n",
      "('A', 'M')\n",
      "('F', 'm')\n",
      "('E', 'm')\n",
      "('E', 'M')\n",
      "('F', 'M')\n",
      "('C', 'M')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'm')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('F', 'm')\n",
      "('G', 'm')\n",
      "('A', 'M')\n",
      "('A', 'M')\n",
      "('D', 'm')\n",
      "('E', 'M')\n",
      "('G', 'M')\n",
      "('E', 'm')\n",
      "('F', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('F', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('A', 'M')\n",
      "('A', 'NC')\n",
      "('D', 'm')\n",
      "('E', 'M')\n",
      "('G', 'M')\n",
      "('F', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'm')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('E', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('E', 'm')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('G', 'M')\n",
      "('A', 'm')\n",
      "('C', 'M')\n",
      "('D', 'M')\n",
      "('C', 'M')\n",
      "('F', 'M')\n",
      "('C', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('A', 'M')\n",
      "('A', 'm')\n",
      "('C', 'M')\n",
      "('A', 'M')\n",
      "('G', 'm')\n",
      "('F', 'M')\n",
      "('F', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('F', 'M')\n",
      "('G', 'm')\n",
      "('D', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('A', 'm')\n",
      "('A', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('F', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'm')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('G', 'm')\n",
      "('G', 'M')\n",
      "('A', 'M')\n",
      "('D', 'M')\n",
      "('C', 'M')\n",
      "('G', 'M')\n",
      "('E', 'm')\n",
      "('F', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('F', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('A', 'M')\n",
      "('D', 'm')\n",
      "('E', 'm')\n",
      "('G', 'M')\n",
      "('F', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'm')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('E', 'm')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('A', 'm')\n",
      "('F', 'M')\n",
      "('B', 'm')\n",
      "('E', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'm')\n",
      "('G', 'NC')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('E', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('E', 'm')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('G', 'M')\n",
      "('A', 'm')\n",
      "('C', 'M')\n",
      "('D', 'M')\n",
      "('C', 'M')\n",
      "('F', 'M')\n",
      "('C', 'M')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('A', 'M')\n",
      "('A', 'M')\n",
      "('B', 'm')\n",
      "('E', 'M')\n",
      "('A', 'M')\n",
      "('G', 'M')\n",
      "('F', 'M')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('A', 'm')\n",
      "('A', 'm')\n",
      "('A', 'M')\n",
      "('G', 'M')\n",
      "('F', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('G', 'm')\n",
      "('G', 'M')\n",
      "('F', 'M')\n",
      "('C', 'm')\n",
      "('E', 'M')\n",
      "('G', 'M')\n",
      "('E', 'm')\n",
      "('F', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('F', 'M')\n",
      "('G', 'M')\n",
      "('E', 'm')\n",
      "('C', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('A', 'M')\n",
      "('D', 'm')\n",
      "('E', 'm')\n",
      "('A#', 'm')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('F', 'M')\n",
      "('C', 'm')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('E', 'm')\n",
      "('G', 'M')\n",
      "('G', 'M')\n",
      "('A', 'm')\n",
      "('F', 'M')\n",
      "('B', 'm')\n",
      "('E', 'M')\n",
      "('C', 'm')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'm')\n",
      "('G', 'NC')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('A', 'M')\n",
      "('D', 'M')\n",
      "('A', 'M')\n",
      "('A', 'M')\n",
      "('A', 'm')\n",
      "('E', 'm')\n",
      "('G', 'M')\n",
      "('F', 'M')\n",
      "('F', 'M')\n",
      "('F', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('A', 'M')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('G', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('C', 'M')\n",
      "('D', 'NC')\n",
      "('G', 'm')\n"
     ]
    }
   ],
   "source": [
    "play_predicts('test/Maroon_5_-_Memories.wav', mode=5)\n",
    "# play_check(\"data/losing_my_religion.wav\", \"data/losing_my_religion.csv\", fast=True, mode=3)\n",
    "\n",
    "# play_check('data/All_Of_Me.wav','data/All_Of_Me.csv', fast=True, mode=3)\n",
    "# play_check('data/All_Of_Me.wav','data/All_Of_Me.csv', fast=True, mode=4)\n",
    "\n",
    "# play_check(\"data/chordtest0.wav\", \"data/chordtest0.csv\", fast=True, mode=4)\n",
    "# play_check(\"data/chordtest0.wav\", \"data/chordtest0.csv\", fast=True, mode=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import time\n",
    "\n",
    "def my_function(x):\n",
    "    return x\n",
    "# create a slider\n",
    "slider = interact(my_function, x=20)\n",
    "\n",
    "def incrementtime():\n",
    "    count = 0\n",
    "    while count<60:\n",
    "        time.sleep(1000)\n",
    "        slider.value += 1\n",
    "        count+=1\n",
    "        \n",
    "incrementtime()\n",
    "print(slider.value)\n",
    "\n",
    "def increase_value_every_t_sec(initail_value, interval, increase_by,stop_after = -1):\n",
    "    counter = 0\n",
    "    values = []\n",
    "    while counter < stop_after or stop_after == -1:\n",
    "        time.sleep(interval)\n",
    "        initail_value += increase_by\n",
    "        print(initail_value)\n",
    "        values.append(initail_value)\n",
    "        counter += 1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "aa = ipd.Audio('test/Maroon_5_-_Memories.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658fe3aa057742e1943cd8763743b832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='x', max=60, min=-20), Output()), _dom_classes=('widget-"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.my_function(x)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Updated:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.widget_value = None\n",
    "        self.derived_value = None\n",
    "\n",
    "    def update(self, val_dict) -> None:\n",
    "        self.widget_value = val_dict['new']\n",
    "        self.derived_value = self.widget_value + 10\n",
    "\n",
    "update_class = Updated()\n",
    "\n",
    "x = 5\n",
    "y = []\n",
    "slider = widgets.IntSlider()\n",
    "slider.value = x\n",
    "\n",
    "def on_change(v):\n",
    "    update_class.update(v)\n",
    "slider.observe(on_change, names='value')\n",
    "display(slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://musicinformationretrieval.com/ipython_audio.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
